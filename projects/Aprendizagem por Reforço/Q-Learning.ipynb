{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gym OpenAI] Nesse tipo de aprendizagem, existe um ambiente que possui um objetivo de otimização e existe um agente neste ambiente que varia estados com base em recompensas (positivas ou negativas). Se ele se aproximar do objetivo, recebe uma recompensa positiva e se afastar do objetivo, a recompensa é negativa.<br><br>\n",
    "**táxi**<br>\n",
    "Movimentos e Recompensas:\n",
    "1. Cima;\n",
    "2. Baixo;\n",
    "3. Frente;\n",
    "4. Trás;\n",
    "5. Pegar passageiro;\n",
    "6. Deixar passageiro.\n",
    "* -10: Buscar o passageiro no lugar errado;\n",
    "* -10: Deixar o passageiro no lugar errado;\n",
    "* -1: Andar no ambiente (living penalty);\n",
    "* +20: Buscar e deixar o passageiro no lugar certo.<br>\n",
    "\n",
    "**Equação de Bellman**:\n",
    "$$V(s)=max_a \\left( R(s,a)+\\gamma V(s') \\right)$$\n",
    "Nesta equação, cada $V(s)$ é um estado (bloquinho) que fornece uma recomensa $R(s,a)$ com $a$ sendo a ação e $s$ o novo estado e adicionamos ao $V(s')$ que se refere ao próximo estado, somado de um parâmetro $\\gamma$ (valor de desconto, ex.: 0.9).<br><br>\n",
    "**Markov Decision Processo (MDP)**:<br>\n",
    "Refere-se às opções de operar sob um cenário de Exploitation determinístico em que o agente sempre irá para  amelhor posição do ambiente ou de Exploration não determinístico em que há uma maior probebilidade de ele seguir para a melhor posição.<br><br>\n",
    "**Q-Learning**<br>\n",
    "É um algoritmo que consiste em uma fórmula matemária para decidir a melhor posição (o agente segue para o valor de maior posição $Q(s_0,\\,a_n)$ em que $Q$ pode ser entendido como qualidade, que ocorre antes ao valor de $V(s)$ de Bellman).<br><br>\n",
    "**Diferença Temporal**:<br>\n",
    "Todas as vezes que o agente interage com o ambiente, o algoritmo executa um episódio (rodada completa de execução) e retorna um valor que indicarão a qualidade de cada uma das ações.\n",
    "$$Q_t(s,a)=Q_{t-1}(s,a)+\\alpha \\left( R(s,a)+\\gamma V(s') \\right)$$\n",
    "O parâmetro $\\alpha$ é a taxa de aprendizagem (learning rate) que indica o quão rápido o algoritmo aprenderá. Outra forma de escrever esta equação é a seguinte:\n",
    "$$Q_t(s,a)=Q_{t-1}(s,a)+\\alpha TD_t(a,s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install gym`<br>\n",
    "`pip install pygame`<br>\n",
    "`pip install `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_ansi_escape_sequences(text):\n",
    "    ansi_escape = re.compile(r'(?:\\x1B[@-_][0-?]*[ -/]*[@-~])')\n",
    "    return ansi_escape.sub('', text)\n",
    "\n",
    "env = gym.make('Taxi-v3', render_mode=\"ansi\")\n",
    "env.reset()\n",
    "output = env.render()\n",
    "clean_output = remove_ansi_escape_sequences(output)\n",
    "print(clean_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# 0 = baixo; 1 = north; 2 = east; 3 = west; 4 = pickup; 5 = dropoff\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "# 4 destinations\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavomeira/.pyenv/versions/3.12.0/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "len(env.P);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 484, -1, False)],\n",
       " 1: [(1.0, 384, -1, False)],\n",
       " 2: [(1.0, 484, -1, False)],\n",
       " 3: [(1.0, 464, -1, False)],\n",
       " 4: [(1.0, 484, -10, False)],\n",
       " 5: [(1.0, 484, -10, False)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(env.P[484]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "display(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:18\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "for i in range(10000):\n",
    "    estado = env.reset()\n",
    "\n",
    "    penalidades, recompensa = 0, 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            # Exploration\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploitation\n",
    "            action = np.argmax(q_table[estado])\n",
    "\n",
    "        proximo_estado, recompensa, done, info = env.step(action)\n",
    "\n",
    "        q_antigo = q_table[estado, action]\n",
    "        proximo_maximo = np.max(q_table[(proximo_estado),:])\n",
    "\n",
    "        q_novo = (1-alpha)*q_antigo +alpha*(recompensa + gamma*proximo_maximo)\n",
    "        q_table[estado,action] = q_novo\n",
    "\n",
    "        if recompensa == -10:\n",
    "            penalidades += 1\n",
    "\n",
    "        estado = proximo_estado\n",
    "\n",
    "    if i%100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print('Epsilon =',i)\n",
    "\n",
    "print('Concluído')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
