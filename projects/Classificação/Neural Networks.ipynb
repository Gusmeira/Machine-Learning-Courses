{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Redes Neurais Artificiais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base de Crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/gustavomeira/Documents/Python/Estudo_ML/projects/Classificação/credit.pkl',mode='rb') as f:\n",
    "    [X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste] = pickle.load(f)\n",
    "    \n",
    "X_credit_treinamento = np.array(X_credit_treinamento)\n",
    "y_credit_treinamento = np.array(y_credit_treinamento)\n",
    "X_credit_teste = np.array(X_credit_teste)\n",
    "y_credit_teste = np.array(y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavomeira/.pyenv/versions/3.12.0/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "neural_network_credit = MLPClassifier(max_iter=1500,\n",
    "                                      verbose=False,\n",
    "                                      tol=0.00001,\n",
    "                                      solver='adam',\n",
    "                                      activation='relu',\n",
    "                                      hidden_layer_sizes=(2,2),\n",
    "                                      random_state=0);\n",
    "neural_network_credit.fit(X=X_credit_treinamento, y=y_credit_treinamento);\n",
    "\n",
    "previsoes = neural_network_credit.predict(X=X_credit_teste);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998 \n",
      "\n",
      "[[435   1]\n",
      " [  0  64]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_credit_teste,previsoes),'\\n')\n",
    "print(confusion_matrix(y_credit_teste,previsoes),'\\n')\n",
    "print(classification_report(y_credit_teste,previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base do Censo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/gustavomeira/Documents/Python/Estudo_ML/projects/Classificação/census.pkl',mode='rb') as f:\n",
    "    [X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste] = pickle.load(f)\n",
    "\n",
    "X_census_treinamento = np.array(X_census_treinamento)\n",
    "y_census_treinamento = np.array(y_census_treinamento)\n",
    "X_census_teste = np.array(X_census_teste)\n",
    "y_census_teste = np.array(y_census_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_census = MLPClassifier(max_iter=1000,\n",
    "                                      verbose=False,\n",
    "                                      tol=0.00001,\n",
    "                                      solver='adam',\n",
    "                                      activation='relu',\n",
    "                                      hidden_layer_sizes=(55,55),\n",
    "                                      random_state=0)\n",
    "neural_network_census.fit(X=X_census_treinamento, y=y_census_treinamento)\n",
    "\n",
    "previsoes = neural_network_census.predict(X=X_census_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8204708290685773 \n",
      "\n",
      "[[3285  408]\n",
      " [ 469  723]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.89      0.88      3693\n",
      "        >50K       0.64      0.61      0.62      1192\n",
      "\n",
      "    accuracy                           0.82      4885\n",
      "   macro avg       0.76      0.75      0.75      4885\n",
      "weighted avg       0.82      0.82      0.82      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_census_teste,previsoes),'\\n')\n",
    "print(confusion_matrix(y_census_teste,previsoes),'\\n')\n",
    "print(classification_report(y_census_teste,previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron:**<br>\n",
    "Cada neurônio recebe uma informação $x_{i}$ e é passada por um axônio com um peso $w_{i}$ correspondente. No final, a função soma captura esse resultado $\\sum_{i=1}^{n}{x_{i}.w_{i}}$. A partir daí, aplicamos esse valor em uma função de ativação (step function) que indica se esse valor permite ou não a ativação desse neurônio.\n",
    "Os pesos são as sinapses e podem ser positivos (excitadores) ou negativos (inibidores). Eles amplificam ou reduzem o sinal emitido pelas informações, sendo eles os principais outputs de ajustes de uma rede neural (queremos determinar quais os melhores pesos a serem aprendidos).\n",
    "<br> <br> \n",
    "\n",
    "**Ajuste dos Pesos:**<br>\n",
    "O erro é calculado por: $E = R_{correta}-R_{calculada}$<br>\n",
    "Atualização dos pesos: $Peso(n+1)=Peso(n)+(taxa_{aprendizagem}\\,.\\,Input\\,.\\,E)$<br>\n",
    "Outros erros são:\n",
    "$$MSE=\\frac{1}{N}\\,\\sum_{i=1}^{N}(f_{i}-y_{i})^2$$\n",
    "$$RMSE=\\sqrt{\\frac{1}{N}\\,\\sum_{i=1}^{N}(f_{i}-y_{i})^2}$$\n",
    "<br>\n",
    "\n",
    "**Funções de Ativação:**<br>\n",
    "Um perceptron de uma camada só é válido para problemas linearmente separáveis (como and e or, mas não xor). Há então a implementação de um feed forward (fornecemos a entrada, a rede neural multicamada processa e retorna uma resposta). As funções de ativação depois da função soma podem ser do tipo step $f=0\\,\\text{or}\\,1$, sigmoid $y=1/[1+\\exp{-x}]$, tangente hiperbólica $y=[\\exp{x}-\\exp{-x}]/[\\exp{x}+\\exp{-x}]$.\n",
    "<br> <br> \n",
    "\n",
    "**Gradiente Descendente:**<br>\n",
    "A técnica de gradiente descendente envolve a minimização do erro a partir da minimização da função custo por meio das suas derivadas parciais para achar o mínimo global.<br>\n",
    "* Batch: calcula o erro para toda a base de dados;<br>\n",
    "* Stochastic: calcula o erro para cada elemento na base de dados, sendo mais rápido por exigir menos mémoria alocada e ajuda a previnir mínimos locais (superfícies não convexas);<br>\n",
    "* Minibatch: calcula o número de registros para atuar em batch.<br>\n",
    "\n",
    "*Cálculo do Delta:*<br>\n",
    "Delta de saída: $\\Delta_{saída}=E-dy/dx_{sigmoid}$<br>\n",
    "Delta oculto: $\\Delta_{oculto}=dy/dx_{sigmoid}\\,.\\,w_{peso}\\,.\\,\\Delta_{saída}$<br>\n",
    "O número de camadas ocultas, em geral, pode ser descrito pela seguinte fórmula (para problemas não linearmente separáveis): $$n=\\frac{Input+Output}{2}$$<br>\n",
    "\n",
    "*Backpropagation:*<br>\n",
    "É o processo inverso do feed forward pois ele retro propaga o <br>\n",
    "Backpropagation: \n",
    "$peso_{n_{new}}=(peso_{n}\\,.\\,momento)+(entrada\\,.\\,\\Delta\\,.\\,taxa_{aprendizagem})$<br>\n",
    "\n",
    "*Learning Rate:*<br>\n",
    "* Define o quão rápido o aloritmo vai aprender, sendo que o alto apresenta uma convergência mais rápida mas pode ficar preso em um mínimo local.<br>\n",
    "\n",
    "*Momentum:*<br>\n",
    "* Visa escapar de mínimos locais, sendo que define o quão confiável é a última alteração. Um valor alto aumenta a velocidade de convergência mas um valor baixo escapa melhor de mínimos locais.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
